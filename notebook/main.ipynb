{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c05df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import os\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1568440",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NoSuchMethodError: 'void org.apache.spark.internal.LogKey.$init$(org.apache.spark.internal.LogKey)'\r\n\tat org.apache.spark.internal.LogKeys$SPARK_VERSION$.<clinit>(LogKey.scala:763)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$1(SparkContext.scala:203)\r\n\tat org.apache.spark.internal.LogEntry.cachedMessageWithContext$lzycompute(Logging.scala:102)\r\n\tat org.apache.spark.internal.LogEntry.cachedMessageWithContext(Logging.scala:102)\r\n\tat org.apache.spark.internal.LogEntry.context(Logging.scala:106)\r\n\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:189)\r\n\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:187)\r\n\tat org.apache.spark.SparkContext.logInfo(SparkContext.scala:86)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:203)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#creating spark session\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      4\u001b[39m spark = (\n\u001b[32m      5\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAmazonReviewsSparkAnalytics\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.bindAddress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m127.0.0.1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.sql.warehouse.dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfile:/C:/temp/spark-warehouse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.ui.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfalse\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m\"\u001b[39m\u001b[33mWARN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark version:\u001b[39m\u001b[33m\"\u001b[39m, spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bhara\\OneDrive\\Desktop\\DSML\\amazon-reviews-spark-analytics\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NoSuchMethodError: 'void org.apache.spark.internal.LogKey.$init$(org.apache.spark.internal.LogKey)'\r\n\tat org.apache.spark.internal.LogKeys$SPARK_VERSION$.<clinit>(LogKey.scala:763)\r\n\tat org.apache.spark.SparkContext.$anonfun$new$1(SparkContext.scala:203)\r\n\tat org.apache.spark.internal.LogEntry.cachedMessageWithContext$lzycompute(Logging.scala:102)\r\n\tat org.apache.spark.internal.LogEntry.cachedMessageWithContext(Logging.scala:102)\r\n\tat org.apache.spark.internal.LogEntry.context(Logging.scala:106)\r\n\tat org.apache.spark.internal.Logging.logInfo(Logging.scala:189)\r\n\tat org.apache.spark.internal.Logging.logInfo$(Logging.scala:187)\r\n\tat org.apache.spark.SparkContext.logInfo(SparkContext.scala:86)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:203)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n"
     ]
    }
   ],
   "source": [
    "#creating spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"AmazonReviewsSparkAnalytics\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:/C:/temp/spark-warehouse\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from GitHub...\n",
      "Saved to: data\\AmazonProductReviews.csv\n"
     ]
    }
   ],
   "source": [
    "#Donloading the dataset from github repo\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/bharathkrishna711-lab/Amazon-Review-spark-analytics/main/AmazonProductReviews.csv\"\n",
    "\n",
    "\n",
    "LOCAL_DIR = \"data\"\n",
    "LOCAL_PATH = os.path.join(LOCAL_DIR, \"AmazonProductReviews.csv\")\n",
    "\n",
    "os.makedirs(LOCAL_DIR, exist_ok=True)\n",
    "\n",
    "# Download only if not already downloaded\n",
    "if not os.path.exists(LOCAL_PATH):\n",
    "    print(\"Downloading dataset from GitHub...\")\n",
    "    urllib.request.urlretrieve(DATASET_URL, LOCAL_PATH)\n",
    "    print(\"Saved to:\", LOCAL_PATH)\n",
    "else:\n",
    "    print(\"Dataset already exists at:\", LOCAL_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7cbef3",
   "metadata": {},
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e41328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records loaded: 1597\n",
      "=== Raw Schema ===\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- asins: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- colors: string (nullable = true)\n",
      " |-- dateAdded: timestamp (nullable = true)\n",
      " |-- dateUpdated: timestamp (nullable = true)\n",
      " |-- dimension: string (nullable = true)\n",
      " |-- ean: double (nullable = true)\n",
      " |-- keys: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- manufacturerNumber: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- prices: string (nullable = true)\n",
      " |-- reviews.date: timestamp (nullable = true)\n",
      " |-- reviews.doRecommend: boolean (nullable = true)\n",
      " |-- reviews.numHelpful: integer (nullable = true)\n",
      " |-- reviews.rating: integer (nullable = true)\n",
      " |-- reviews.sourceURLs: string (nullable = true)\n",
      " |-- reviews.text: string (nullable = true)\n",
      " |-- reviews.title: string (nullable = true)\n",
      " |-- reviews.userCity: string (nullable = true)\n",
      " |-- reviews.userProvince: string (nullable = true)\n",
      " |-- reviews.username: string (nullable = true)\n",
      " |-- sizes: string (nullable = true)\n",
      " |-- upc: double (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n",
      "Columns: ['id', 'asins', 'brand', 'categories', 'colors', 'dateAdded', 'dateUpdated', 'dimension', 'ean', 'keys', 'manufacturer', 'manufacturerNumber', 'name', 'prices', 'reviews.date', 'reviews.doRecommend', 'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs', 'reviews.text', 'reviews.title', 'reviews.userCity', 'reviews.userProvince', 'reviews.username', 'sizes', 'upc', 'weight']\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "df_raw = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .csv(LOCAL_PATH)\n",
    ")\n",
    "\n",
    "#printing no:of records loaded\n",
    "print(\"Total records loaded:\", df_raw.count())\n",
    "\n",
    "print(\"=== Raw Schema ===\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Columns:\", df_raw.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5dc5d4",
   "metadata": {},
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc2a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Clean Schema ===\n",
      "root\n",
      " |-- categories: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- reviews_date: timestamp (nullable = true)\n",
      " |-- reviews_rating: integer (nullable = true)\n",
      " |-- reviews_text: string (nullable = true)\n",
      " |-- reviews_title: string (nullable = true)\n",
      " |-- reviews_username: string (nullable = true)\n",
      " |-- primary_category: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      "\n",
      "Records after cleaning: 1177\n"
     ]
    }
   ],
   "source": [
    "# Select required columns \n",
    "df = df_raw.select(\n",
    "    \"categories\",\n",
    "    \"name\",\n",
    "    F.col(\"`reviews.date`\").alias(\"reviews_date\"),\n",
    "    F.col(\"`reviews.rating`\").alias(\"reviews_rating\"),\n",
    "    F.col(\"`reviews.text`\").alias(\"reviews_text\"),\n",
    "    F.col(\"`reviews.title`\").alias(\"reviews_title\"),\n",
    "    F.col(\"`reviews.username`\").alias(\"reviews_username\")\n",
    ")\n",
    "\n",
    "# primary_category = first category in comma-separated categories\n",
    "df = df.withColumn(\"primary_category\", F.trim(F.split(F.col(\"categories\"), \",\").getItem(0)))\n",
    "\n",
    "# cast rating to numeric\n",
    "df = df.withColumn(\"rating\", F.col(\"reviews_rating\").cast(\"double\"))\n",
    "\n",
    "# parse date \n",
    "df = df.withColumn(\"review_date\", F.to_date(F.col(\"reviews_date\")))\n",
    "\n",
    "# drop invalid ratings \n",
    "df_clean = df.filter(\n",
    "    (F.col(\"rating\").isNotNull()) &\n",
    "    (F.col(\"rating\") >= 1.0) &\n",
    "    (F.col(\"rating\") <= 5.0)\n",
    ")\n",
    "\n",
    "print(\"=== Clean Schema ===\")\n",
    "df_clean.printSchema()\n",
    "print(\"Records after cleaning:\", df_clean.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc4075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-------------------+--------------+--------------------+--------------------+--------------------+----------------+------+-----------+\n",
      "|          categories|             name|       reviews_date|reviews_rating|        reviews_text|       reviews_title|    reviews_username|primary_category|rating|review_date|\n",
      "+--------------------+-----------------+-------------------+--------------+--------------------+--------------------+--------------------+----------------+------+-----------+\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-08-08 05:30:00|             5|I initially had t...|Paperwhite voyage...|          Cristina M|  Amazon Devices|   5.0| 2015-08-08|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-09-01 05:30:00|             5|Allow me to prefa...|One Simply Could ...|               Ricky|  Amazon Devices|   5.0| 2015-09-01|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-20 05:30:00|             4|I am enjoying it ...|Great for those t...|       Tedd Gardiner|  Amazon Devices|   4.0| 2015-07-20|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2017-06-16 05:30:00|             5|I bought one of t...|Love / Hate relat...|              Dougal|  Amazon Devices|   5.0| 2017-06-16|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2016-08-11 05:30:00|             5|I have to say upf...|           I LOVE IT|  Miljan David Tanic|  Amazon Devices|   5.0| 2016-08-11|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-08 05:30:00|          NULL|My previous kindl...|Great device for ...|          Kelvin Law|  Amazon Devices|  NULL| 2015-07-08|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-09-01 05:30:00|          NULL|Allow me to prefa...|One Simply Could ...|               Ricky|  Amazon Devices|  NULL| 2015-09-01|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-03 05:30:00|          NULL|Just got mine rig...|Definitely better...|             Bandler|  Amazon Devices|  NULL| 2015-07-03|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-08-08 05:30:00|          NULL|I initially had t...|Paperwhite voyage...|          Cristina M|  Amazon Devices|  NULL| 2015-08-08|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-20 05:30:00|          NULL|I am enjoying it ...|Great for those t...|       Tedd Gardiner|  Amazon Devices|  NULL| 2015-07-20|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-01 05:30:00|          NULL|As reviewed by th...|Give this to a bo...|     Miguel Martinez|  Amazon Devices|  NULL| 2015-07-01|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-08-05 05:30:00|          NULL|My new Kindle Pap...|Trouble-free inte...|   Magnus Brattemark|  Amazon Devices|  NULL| 2015-08-05|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|2015-07-20 05:30:00|          NULL|I am enjoying it ...|Great for those t...|       Tedd Gardiner|  Amazon Devices|  NULL| 2015-07-20|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             4|Had older model, ...|Liked the smaller...|  Janet Matthews Jan|  Amazon Devices|   4.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             5|This is a review ...|Superb reading de...|John Kat's the br...|  Amazon Devices|   5.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             5|I love my kindle!...|          I love it!|              samira|  Amazon Devices|   5.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             4|Vraiment bon peti...|          Un plaisir|        Louis simard|  Amazon Devices|   4.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             5|Exactly what it i...|Works great and I...|              JanetC|  Amazon Devices|   5.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             4|Trs heureux que l...|           Trs utile|            Shepherd|  Amazon Devices|   4.0|       NULL|\n",
      "|Amazon Devices,ma...|Kindle Paperwhite|               NULL|             4|Only 4 stars beca...|I had a Kindle be...|              Brenda|  Amazon Devices|   4.0|       NULL|\n",
      "+--------------------+-----------------+-------------------+--------------+--------------------+--------------------+--------------------+----------------+------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.createOrReplaceTempView(\"reviews\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac015c1",
   "metadata": {},
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+------------+----------+\n",
      "|name                                                 |review_count|avg_rating|\n",
      "+-----------------------------------------------------+------------+----------+\n",
      "|Fire HD 6 Tablet                                     |38          |5.0       |\n",
      "|Kindle Paperwhite                                    |22          |4.591     |\n",
      "|Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker|542         |4.533     |\n",
      "|Kindle Fire HDX 7\"                                   |23          |4.391     |\n",
      "|Amazon Fire TV                                       |44          |4.205     |\n",
      "|Amazon Premium Headphones                            |77          |4.013     |\n",
      "|All-New Amazon Fire 7 Tablet Case (7th Generation    |27          |3.778     |\n",
      "+-----------------------------------------------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#product names with at least 20 reviews and rank them by average rating.\n",
    "q3 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  name,\n",
    "  COUNT(*) AS review_count,\n",
    "  ROUND(AVG(rating), 3) AS avg_rating\n",
    "FROM reviews\n",
    "GROUP BY name\n",
    "HAVING COUNT(*) >= 20\n",
    "ORDER BY avg_rating DESC, review_count DESC, name ASC\n",
    "\"\"\")\n",
    "\n",
    "q3.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49db2f",
   "metadata": {},
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d0a072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|username       |review_count|\n",
      "+---------------+------------+\n",
      "|A. Younan      |38          |\n",
      "|Andrew         |23          |\n",
      "|William Hardin |23          |\n",
      "|Amazon Customer|17          |\n",
      "|Victor L.      |15          |\n",
      "|Earthling1984  |13          |\n",
      "|NF             |13          |\n",
      "|Amazon Reviewer|12          |\n",
      "|Mike W.        |12          |\n",
      "|D. Miyao       |10          |\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 10 most active reviewers. \n",
    "q4 = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  reviews_username AS username,\n",
    "  COUNT(*) AS review_count\n",
    "FROM reviews\n",
    "GROUP BY reviews_username\n",
    "ORDER BY review_count DESC, username ASC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "q4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b49b5",
   "metadata": {},
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e8361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------+----------+------------+\n",
      "|primary_category            |year_month|avg_rating|review_count|\n",
      "+----------------------------+----------+----------+------------+\n",
      "|Amazon Devices              |2012-09   |4.5       |4           |\n",
      "|Amazon Devices              |2012-10   |4.0       |1           |\n",
      "|Amazon Devices              |2013-10   |4.313     |16          |\n",
      "|Amazon Devices              |2013-11   |4.167     |6           |\n",
      "|Amazon Devices              |2013-12   |5.0       |2           |\n",
      "|Amazon Devices              |2014-07   |5.0       |39          |\n",
      "|Amazon Devices              |2014-09   |4.0       |1           |\n",
      "|Amazon Devices              |2014-10   |3.667     |6           |\n",
      "|Amazon Devices              |2014-11   |5.0       |1           |\n",
      "|Amazon Devices              |2015-03   |1.0       |1           |\n",
      "|Amazon Devices              |2015-04   |5.0       |1           |\n",
      "|Amazon Devices              |2015-06   |5.0       |1           |\n",
      "|Amazon Devices              |2015-07   |4.8       |5           |\n",
      "|Amazon Devices              |2015-08   |5.0       |1           |\n",
      "|Amazon Devices              |2015-09   |5.0       |1           |\n",
      "|Amazon Devices              |2015-10   |4.0       |2           |\n",
      "|Amazon Devices              |2016-02   |1.0       |1           |\n",
      "|Amazon Devices              |2016-03   |2.0       |2           |\n",
      "|Amazon Devices              |2016-04   |4.0       |7           |\n",
      "|Amazon Devices              |2016-05   |5.0       |4           |\n",
      "|Amazon Devices              |2016-06   |4.846     |13          |\n",
      "|Amazon Devices              |2016-07   |4.708     |24          |\n",
      "|Amazon Devices              |2016-08   |4.258     |31          |\n",
      "|Amazon Devices              |2016-09   |4.544     |68          |\n",
      "|Amazon Devices              |2016-10   |4.472     |36          |\n",
      "|Amazon Devices              |2016-11   |4.448     |58          |\n",
      "|Amazon Devices              |2016-12   |4.579     |57          |\n",
      "|Amazon Devices              |2017-01   |4.588     |119         |\n",
      "|Amazon Devices              |2017-02   |4.405     |42          |\n",
      "|Amazon Devices              |2017-03   |4.636     |44          |\n",
      "|Amazon Devices              |2017-04   |4.605     |43          |\n",
      "|Amazon Devices              |2017-05   |4.438     |32          |\n",
      "|Amazon Devices              |2017-06   |4.365     |85          |\n",
      "|Amazon Devices              |2017-07   |4.2       |70          |\n",
      "|Amazon Devices & Accessories|2015-05   |3.0       |1           |\n",
      "|Amazon Devices & Accessories|2016-02   |1.667     |3           |\n",
      "|Amazon Devices & Accessories|2016-03   |1.667     |6           |\n",
      "|Amazon Devices & Accessories|2016-04   |3.0       |1           |\n",
      "|Amazon Devices & Accessories|2016-05   |3.0       |2           |\n",
      "|Amazon Devices & Accessories|2016-06   |1.0       |1           |\n",
      "+----------------------------+----------+----------+------------+\n",
      "only showing top 40 rows\n"
     ]
    }
   ],
   "source": [
    "#Monthly Trend of Average Ratings per primary_category of product. Showing 40 rows.\n",
    "q5 = (\n",
    "    df_clean\n",
    "    .filter(F.col(\"review_date\").isNotNull())\n",
    "    .withColumn(\"year_month\", F.date_format(\"review_date\", \"yyyy-MM\"))\n",
    "    .groupBy(\"primary_category\", \"year_month\")\n",
    "    .agg(\n",
    "        F.round(F.avg(\"rating\"), 3).alias(\"avg_rating\"),\n",
    "        F.count(\"*\").alias(\"review_count\")\n",
    "    )\n",
    "    .orderBy(\"primary_category\", \"year_month\")\n",
    ")\n",
    "\n",
    "q5.show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15886ff9",
   "metadata": {},
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------+---------+--------+------------+-------------+\n",
      "|name                                                                                     |five_star|one_star|ratio_5_to_1|total_reviews|\n",
      "+-----------------------------------------------------------------------------------------+---------+--------+------------+-------------+\n",
      "|Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker                                    |358      |7       |51.143      |542          |\n",
      "|Echo Dot (2nd Generation) - Black                                                        |12       |1       |12.0        |13           |\n",
      "|Amazon Fire TV                                                                           |26       |4       |6.5         |44           |\n",
      "|Amazon Fire TV Game Controller                                                           |6        |1       |6.0         |11           |\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|10       |2       |5.0         |19           |\n",
      "|All-New Amazon Fire HD 8 Tablet Case (7th Generation                                     |5        |1       |5.0         |18           |\n",
      "|Amazon Kindle Oasis Premium Leather Battery Cover - Walnut                               |5        |1       |5.0         |12           |\n",
      "|All-New Amazon Fire 7 Tablet Case (7th Generation                                        |14       |4       |3.5         |27           |\n",
      "|Alexa Voice Remote for Amazon Echo and Echo Dot                                          |3        |1       |3.0         |7            |\n",
      "|Kindle Fire HDX 8.9\"                                                                     |2        |1       |2.0         |15           |\n",
      "+-----------------------------------------------------------------------------------------+---------+--------+------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#top 10 Product names by the Ratio of 5-Star to 1-Star Reviews. \n",
    "q6 = (\n",
    "    df_clean\n",
    "    .groupBy(\"name\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"rating\") == 5, 1).otherwise(0)).alias(\"five_star\"),\n",
    "        F.sum(F.when(F.col(\"rating\") == 1, 1).otherwise(0)).alias(\"one_star\"),\n",
    "        F.count(\"*\").alias(\"total_reviews\")\n",
    "    )\n",
    "    .filter(F.col(\"one_star\") > 0)\n",
    "    .withColumn(\"ratio_5_to_1\", F.round(F.col(\"five_star\") / F.col(\"one_star\"), 3))\n",
    "    .orderBy(F.col(\"ratio_5_to_1\").desc(), F.col(\"five_star\").desc(), F.col(\"total_reviews\").desc())\n",
    "    .select(\"name\", \"five_star\", \"one_star\", \"ratio_5_to_1\", \"total_reviews\")\n",
    ")\n",
    "\n",
    "q6.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddbb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|    primary_category|                name|        review_title|review_len|         review_text|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "|          Categories|      Amazon Fire TV|This box is a GAM...|     19739|I am not a casual...|\n",
      "|      Amazon Devices|  Kindle Fire HDX 7\"|Excellent 3rd-gen...|     18667|This is the middl...|\n",
      "|Amazon Devices & ...|Alexa Voice Remot...|Great range, very...|      1925|As other reviewer...|\n",
      "|         Electronics|All-New Fire HD 8...|Fantastic tablet ...|      1778|Let me start by s...|\n",
      "|        Kindle Store|     Kindle Keyboard|Worth the money. ...|      1672|The Kindle is my ...|\n",
      "|Cell Phones & Acc...|Moshi Anti-Glare ...|Moshi's screen pr...|      1379|I like Moshi's an...|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#longest review in each primary category along with review title and length of review text.\n",
    "#Finding length of each review \n",
    "df_len = df_clean.withColumn(\"review_len\", F.length(\"reviews_text\"))\n",
    "\n",
    "w = Window.partitionBy(\"primary_category\").orderBy(F.col(\"review_len\").desc())\n",
    "\n",
    "q7 = (\n",
    "    df_len\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .select(\n",
    "        \"primary_category\",\n",
    "        \"name\",\n",
    "        F.col(\"reviews_title\").alias(\"review_title\"),\n",
    "        \"review_len\",\n",
    "        F.col(\"reviews_text\").alias(\"review_text\")\n",
    "    )\n",
    "    .orderBy(F.col(\"review_len\").desc())\n",
    ")\n",
    "\n",
    "q7.show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbef8e",
   "metadata": {},
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d733a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+---------------+--------------+\n",
      "|year|review_count|prev_year_count|yoy_growth_pct|\n",
      "+----+------------+---------------+--------------+\n",
      "|2012|5           |NULL           |NULL          |\n",
      "|2013|24          |5              |380.0         |\n",
      "|2014|101         |24             |320.83        |\n",
      "|2015|18          |101            |-82.18        |\n",
      "|2016|328         |18             |1722.22       |\n",
      "|2017|484         |328            |47.56         |\n",
      "+----+------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Year-over-Year Growth in Review Counts. \n",
    "df_year = (\n",
    "    df_clean\n",
    "    .filter(F.col(\"review_date\").isNotNull())\n",
    "    .withColumn(\"year\", F.year(\"review_date\"))\n",
    "    .groupBy(\"year\")\n",
    "    .agg(F.count(\"*\").alias(\"review_count\"))\n",
    "    .orderBy(\"year\")\n",
    ")\n",
    "\n",
    "w_year = Window.orderBy(\"year\")\n",
    "\n",
    "q8 = (\n",
    "    df_year\n",
    "    .withColumn(\"prev_year_count\", F.lag(\"review_count\").over(w_year))\n",
    "    .withColumn(\n",
    "        \"yoy_growth_pct\",\n",
    "        F.when(F.col(\"prev_year_count\").isNull(), F.lit(None).cast(\"double\"))\n",
    "         .otherwise(F.round((F.col(\"review_count\") - F.col(\"prev_year_count\")) / F.col(\"prev_year_count\") * 100, 2))\n",
    "    )\n",
    ")\n",
    "\n",
    "q8.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64351e54",
   "metadata": {},
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b90e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+--------------+\n",
      "|len_bucket|review_count|avg_rating|avg_review_len|\n",
      "+----------+------------+----------+--------------+\n",
      "|Short     |35          |4.571     |23.6          |\n",
      "|Medium    |447         |4.481     |109.7         |\n",
      "|Long      |695         |4.271     |1174.6        |\n",
      "+----------+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average Rating by Review Length buckets: Analyzes whether longer reviews tend to be more positive or negative. Use 3 buckets as given below: \n",
    " \n",
    "# Short: length of review text is less than 50. \n",
    "\n",
    "# Medium: length of review text is between 50 and 200 (both inclusive) \n",
    "\n",
    "# Long: length of review text is above 200.\n",
    "\n",
    "\n",
    "df_bucket = df_clean.withColumn(\"review_len\", F.length(\"reviews_text\"))\n",
    "\n",
    "df_bucket = df_bucket.withColumn(\n",
    "    \"len_bucket\",\n",
    "    F.when(F.col(\"review_len\") < 50, \"Short\")\n",
    "     .when((F.col(\"review_len\") >= 50) & (F.col(\"review_len\") <= 200), \"Medium\")\n",
    "     .otherwise(\"Long\")\n",
    ")\n",
    "\n",
    "q9 = (\n",
    "    df_bucket\n",
    "    .groupBy(\"len_bucket\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"review_count\"),\n",
    "        F.round(F.avg(\"rating\"), 3).alias(\"avg_rating\"),\n",
    "        F.round(F.avg(\"review_len\"), 1).alias(\"avg_review_len\")\n",
    "    )\n",
    "    .orderBy(\n",
    "        F.when(F.col(\"len_bucket\") == \"Short\", 1)\n",
    "         .when(F.col(\"len_bucket\") == \"Medium\", 2)\n",
    "         .otherwise(3)\n",
    "    )\n",
    ")\n",
    "\n",
    "q9.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1664d3",
   "metadata": {},
   "source": [
    "#Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b7075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------+-----------+----------+------------------+-----------------+-----------+\n",
      "|name                                                                                     |first_month|last_month|first_avg         |last_avg         |rating_drop|\n",
      "+-----------------------------------------------------------------------------------------+-----------+----------+------------------+-----------------+-----------+\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-03    |2017-07   |4.75              |1.0              |3.75       |\n",
      "|Kindle Fire HDX 8.9\"                                                                     |2013-11    |2015-03   |4.0               |1.0              |3.0        |\n",
      "|Alexa Voice Remote for Amazon Echo and Echo Dot                                          |2016-05    |2016-12   |3.0               |1.0              |2.0        |\n",
      "|Kindle for Kids Bundle with the latest Kindle E-reader                                   |2016-10    |2017-04   |5.0               |3.0              |2.0        |\n",
      "|Replacement Remote for Amazon Fire TV Stick                                              |2015-05    |2017-05   |3.0               |1.0              |2.0        |\n",
      "|All-New Amazon Fire HD 8 Tablet Case (7th Generation                                     |2017-06    |2017-07   |3.8333333333333335|2.5              |1.333      |\n",
      "|Amazon Kindle Oasis Premium Leather Battery Cover - Black                                |2016-10    |2017-04   |5.0               |4.0              |1.0        |\n",
      "|Certified Refurbished Fire HD 10 Tablet                                                  |2016-11    |2017-05   |4.0               |3.0              |1.0        |\n",
      "|Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone                           |2014-07    |2015-08   |2.5               |2.0              |0.5        |\n",
      "|Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker                                    |2016-04    |2017-07   |5.0               |4.545454545454546|0.455      |\n",
      "+-----------------------------------------------------------------------------------------+-----------+----------+------------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 10 product names whose ratings have dropped maximum.\n",
    "\n",
    "df_monthly = (\n",
    "    df_clean\n",
    "    .filter(F.col(\"review_date\").isNotNull())\n",
    "    .withColumn(\"year_month\", F.date_format(\"review_date\", \"yyyy-MM\"))\n",
    "    .groupBy(\"name\", \"year_month\")\n",
    "    .agg(\n",
    "        F.avg(\"rating\").alias(\"monthly_avg_rating\"),\n",
    "        F.count(\"*\").alias(\"monthly_review_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "w_asc = Window.partitionBy(\"name\").orderBy(\"year_month\")\n",
    "w_desc = Window.partitionBy(\"name\").orderBy(F.col(\"year_month\").desc())\n",
    "\n",
    "df_first_last = (\n",
    "    df_monthly\n",
    "    .withColumn(\"first_month\", F.first(\"year_month\").over(w_asc))\n",
    "    .withColumn(\"first_avg\", F.first(\"monthly_avg_rating\").over(w_asc))\n",
    "    .withColumn(\"last_month\", F.first(\"year_month\").over(w_desc))\n",
    "    .withColumn(\"last_avg\", F.first(\"monthly_avg_rating\").over(w_desc))\n",
    "    .groupBy(\"name\")\n",
    "    .agg(\n",
    "        F.first(\"first_month\").alias(\"first_month\"),\n",
    "        F.first(\"last_month\").alias(\"last_month\"),\n",
    "        F.first(\"first_avg\").alias(\"first_avg\"),\n",
    "        F.first(\"last_avg\").alias(\"last_avg\")\n",
    "    )\n",
    "    .withColumn(\"rating_drop\", F.round(F.col(\"first_avg\") - F.col(\"last_avg\"), 3))\n",
    ")\n",
    "\n",
    "q10 = df_first_last.orderBy(F.col(\"rating_drop\").desc(), F.col(\"name\").asc()).limit(10)\n",
    "q10.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35f698",
   "metadata": {},
   "source": [
    "#Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce7d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected product for analysis: Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders\n",
      "+-----------------------------------------------------------------------------------------+----------+------------------+--------------------+\n",
      "|name                                                                                     |year_month|monthly_avg_rating|monthly_review_count|\n",
      "+-----------------------------------------------------------------------------------------+----------+------------------+--------------------+\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-03   |4.75              |4                   |\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-04   |2.6666666666666665|3                   |\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-05   |4.4               |5                   |\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-06   |4.75              |4                   |\n",
      "|Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders|2017-07   |1.0               |1                   |\n",
      "+-----------------------------------------------------------------------------------------+----------+------------------+--------------------+\n",
      "\n",
      "+-----------+------+--------------------+--------------------+\n",
      "|review_date|rating|       reviews_title|        reviews_text|\n",
      "+-----------+------+--------------------+--------------------+\n",
      "| 2017-07-02|   1.0|It DOESN'T Work O...|My adaptor doesn'...|\n",
      "| 2017-04-20|   1.0|Only lasts for le...|This is the THIRD...|\n",
      "| 2017-04-03|   2.0|           Two Stars|                 Too|\n",
      "|       NULL|   2.0|           Two Stars|    very overpriced!|\n",
      "|       NULL|   2.0|         5W or 5V 1A|Q: What's the dif...|\n",
      "+-----------+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selecting top product whose ratings dropped maximum\n",
    "top_product = q10.collect()[0][\"name\"]\n",
    "print(\"Selected product for analysis:\", top_product)\n",
    "\n",
    "# Monthly trend\n",
    "df_monthly.filter(F.col(\"name\") == top_product).orderBy(\"year_month\").show(60, truncate=False)\n",
    "\n",
    "# Recent low reviews\n",
    "(\n",
    "    df_clean\n",
    "    .filter((F.col(\"name\") == top_product) & (F.col(\"rating\") <= 2))\n",
    "    .select(\"review_date\", \"rating\", \"reviews_title\", \"reviews_text\")\n",
    "    .orderBy(F.col(\"review_date\").desc())\n",
    "    .show(10, truncate=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd2eaa",
   "metadata": {},
   "source": [
    "The product Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders shows a clear decline in customer satisfaction over time. Monthly average ratings were consistently high (above 4.4) from March to June 2017, indicating strong initial performance. However, in July 2017 the average rating dropped sharply to 1.0, signaling a sudden quality or performance issue. Analysis of recent low-rating reviews reveals recurring complaints such as the charger not working, short lifespan, repeated failures, confusion about power specifications, and poor value for money. This pattern suggests a possible manufacturing defect, supplier change, or mismatch between advertised and actual product specifications introduced in later batches. It is recommended that Amazon investigate production or sourcing changes around mid-2017, improve quality checks, and clarify product specifications to restore customer trust and prevent further rating decline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349878fb",
   "metadata": {},
   "source": [
    "#Q12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify a query where there is a performance bottleneck in your Spark job and propose and implement optimizations. \n",
    "# Compare the execution times before and after optimization, and justify your approach. \n",
    "\n",
    "#defining function to check the performance\n",
    "def time_action(label, fn):\n",
    "    start = time.perf_counter()\n",
    "    result = fn()\n",
    "    end = time.perf_counter()\n",
    "    print(f\"{label}: {end - start:.3f} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb32b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE optimization (Qvii) count(): 0.295 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before optimization\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "df_len_base = df_clean.withColumn(\"review_len\", F.length(\"reviews_text\"))\n",
    "w_base = Window.partitionBy(\"primary_category\").orderBy(F.col(\"review_len\").desc())\n",
    "\n",
    "q7_base = (\n",
    "    df_len_base\n",
    "    .withColumn(\"rn\", F.row_number().over(w_base))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .select(\"primary_category\", \"name\", \"reviews_title\", \"review_len\")\n",
    ")\n",
    "\n",
    "time_action(\"BEFORE optimization (Qvii) count()\", lambda: q7_base.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de661c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER optimization (Qvii) count(): 0.160 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After optimization\n",
    "df_clean.cache()\n",
    "df_clean.count()  # materialize cache\n",
    "\n",
    "df_q7 = df_clean.select(\"primary_category\", \"name\", \"reviews_title\", \"reviews_text\")\n",
    "df_q7 = df_q7.repartition(\"primary_category\")  # helps shuffle for window\n",
    "df_q7 = df_q7.withColumn(\"review_len\", F.length(\"reviews_text\"))\n",
    "\n",
    "w_opt = Window.partitionBy(\"primary_category\").orderBy(F.col(\"review_len\").desc())\n",
    "\n",
    "q7_opt = (\n",
    "    df_q7\n",
    "    .withColumn(\"rn\", F.row_number().over(w_opt))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .select(\"primary_category\", \"name\", \"reviews_title\", \"review_len\")\n",
    ")\n",
    "\n",
    "time_action(\"AFTER optimization (Qvii) count()\", lambda: q7_opt.count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Amazon Spark)",
   "language": "python",
   "name": "amazon-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
